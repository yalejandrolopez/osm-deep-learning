library(openeo)
library(raster)
library(sf)
library(geojsonsf)
library(rjson)
library(ggplot2)
con = connect(host = "https://openeo.cloud")
# Collections
collections = list_collections()
login()
# Collections
collections = list_collections()
processes = list_processes()
p = processes()
w = 6.09
s = 46.15
e = 6.99
n = 46.57
date1 = "2018-07-01"
date2 = "2018-07-31"
## cloud cover value (>=)
value = 0.5
# acquire data for the extent
datacube_no2 = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=c(date1, date2),
bands=c("NO2")
)
datacube = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=c(date1, date2),
bands=c("CLOUD_FRACTION")
)
# 10km x 10km grid : may be optional
datacube = p$resample_spatial(
data = datacube, resolution = 10/111
)
datacube_no2 = p$resample_spatial(
data = datacube_no2, resolution = 10/111
)
# mask for cloud cover
threshold_ <- function(data, context) {
threshold <- p$gte(data[1], value)
return(threshold)
}
# apply the threshold to the cube
cloud_threshold <- p$apply(data = datacube, process = threshold_)
# mask the cloud cover with the calculated mask
datacube <- p$mask(datacube_no2, cloud_threshold)
# interpolate where nodata
interpolate = function(data,context) {
return(p$array_interpolate_linear(data = data))
}
datacube = p$apply_dimension(process = interpolate,
data = datacube, dimension = "t"
)
###
# moving average
moving_average <- function(data, context) {
return(p$filter_temporal(data = data, dimension = "t"))
}
datacube = p$apply_dimension(process = moving_average,
data = datacube, dimension = "t"
)
###
# apply dimension to time
datacube = p$apply_dimension(data = datacube, dimension = "t")
# compress spatial dimension
lon = c(w, e)
lat = c(s, n)
bbox_df = data.frame(lon, lat)
pol = st_polygon(
list(
cbind(
bbox_df$lon[c(1,2,2,1,1)],
bbox_df$lat[c(1,1,2,2,1)])
)
)
polygons = st_sfc(pol, crs=4326)
polygons = st_sf(polygons)
# add any attribute as a workaround
polygons$anAttribute <- 4
# aggregate spatially
datacube_mean <- p$aggregate_spatial(data = datacube, reducer = function(data, context) { p$mean(data) }, geometries = polygons)
datacube_max <- p$aggregate_spatial(data = datacube, reducer = function(data, context) { p$max(data) }, geometries = polygons)
# graph results
if ( (((e-w)+(n-s)) * 111) < 350 ){
graph = as(datacube_mean,"Graph")
compute_result(graph = graph, output_file = "time-series-mean.json")
print("mean time series stored \n")
graph = as(datacube_max,"Graph")
compute_result(graph = graph, output_file = "time-series-max.json")
print("max time series stored \n")
# read json - add save option
ts_mean = fromJSON(file = "time-series-mean.json")
ts_max = fromJSON(file = "time-series-max.json")
}else{
print("queuing computation... \n")
print("it may take a while... \n")
formats = list_file_formats()
result = p$save_result(data = datacube_mean,
format = formats$output$JSON)
job = create_job(graph=result, title = "time-series-mean")
start_job(job = job)
while (jobs[[job$id]]$status == 'running' | jobs[[job$id]]$status == 'queued' | jobs[[job$id]]$status == 'created' ){
print(paste0('this may take a moment, your process is ', jobs[[job$id]]$status))
Sys.sleep(60)
if (jobs[[job$id]]$status == 'finished'){
download_results(job = job, folder = "data/")
}
if (jobs[[job$id]]$status == 'finished') {
print('error!')
break
}
}
#jobs # printed as a tibble or data.frame, but the object is a list
#jobs$"vito-19677bea-488a-4c45-a197-de8bf8211020"
#openeo::log_job("vito-19677bea-488a-4c45-a197-de8bf8211020")
#describe_job(job = job)
#list_results(job = job)
result = p$save_result(data = datacube_max,
format = formats$output$JSON)
job = create_job(graph=result, title = "time-series-max")
start_job(job = job)
while (jobs[[job$id]]$status == 'running' | jobs[[job$id]]$status == 'queued' | jobs[[job$id]]$status == 'created' ){
print(paste0('this may take a moment, your process is ', jobs[[job$id]]$status))
Sys.sleep(60)
if (jobs[[job$id]]$status == 'finished'){
download_results(job = job, folder = "data/")
}
if (jobs[[job$id]]$status == 'finished') {
print('error!')
break
}
}
# no2 mean
no2 = list(range(length(ts_mean)))
for (i in 1:length(ts_mean)){
no2[i] = ts_mean[[i]]
}
# no2 max
no2_max = list(range(length(ts_max)))
for (i in 1:length(ts_max)){
no2_max[i] = ts_max[[i]]
}
no2 = unlist(no2)
no2_max = unlist(no2_max)
time = seq(as.Date(date1), by = "days", length.out=length(no2))
no2_k = ksmooth(time(time),no2,'normal',bandwidth=3)
time = seq(as.Date(date1), as.Date(date2), length.out=length(no2_k$x))
scientific_notation <- function(l) {
l <- format(l, scientific = TRUE)
l <- gsub("^(.*)e", "'\\1'e", l)
l <- gsub("e", "%*%10^", l)
parse(text=l)
}
time = c(time,
seq(as.Date(date1), as.Date(date2), length.out=length(no2)),
seq(as.Date(date1), as.Date(date2), length.out=length(no2_max)))
data = c(no2_k$y, no2, no2_max)
group = c(rep("smoothed", length(no2_k$y)),
rep("raw", length(no2)),
rep("maximum", length(no2_max)))
df = data.frame(time, data, group)
png('time-series-no2.png')
ggplot(df, aes(x=time, y=data, group = group, col = group, linetype = group)) +
geom_line() +
scale_y_continuous(labels=scientific_notation) +
theme(legend.position="top") +
theme(legend.title = element_blank())+
xlab("Time") + ylab("Tropospheric NO2 Vertical Column (molec/cmÂ²)")
dev.off()
####
############
#plot(no2, type = "l", xlab = "Time", ylab = c())
#lines(no2_k, col = "red", lty = 22, lwd = 3)
# remove
#min_reducer = function(data,context) {
# return(p$min(data = data))
#}
#reduced = p$reduce_dimension(data = datacube, reducer = min_reducer, dimension="t")
#reduced
# 10km x 10km grid
# threshold for quality flag and cloud cover
# compute daily 30-day smoothed values (kernel smoothing) - missing from the side of the clowd
# Interaction with daily, colour-coded maps (interactive + time-animation)
# display of time series of smoothed,raw, and 30-day maximum values for locations selected
# by user mouse clicks
# comparison of time series of user-selected hot spots with time series of averages for
# selected regions or for global averages. (this bbox or geojson compared to continent, to country...) + compare
#formats = list_file_formats()
#result = p$save_result(data = datacube, format = formats$output$GTiff)
#result
# Batch Job Management
#job = create_job(graph=result, title = "Example Title")
#start_job(job = job)
#jobs = list_jobs()
#jobs # printed as a tibble or data.frame, but the object is a list
#jobs$"vito-19677bea-488a-4c45-a197-de8bf8211020"
#openeo::log_job("vito-19677bea-488a-4c45-a197-de8bf8211020")
#describe_job(job = job)
#list_results(job = job)
#download_results(job = job, folder = "data/")
)
## time extent
date1 = "2018-07-01"
date2 = "2018-07-31"
## cloud cover value (>=)
value = 0.5
# acquire data for the extent
datacube_no2 = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=c(date1, date2),
bands=c("NO2")
)
datacube = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=c(date1, date2),
bands=c("CLOUD_FRACTION")
)
# 10km x 10km grid : may be optional
datacube = p$resample_spatial(
data = datacube, resolution = 10/111
)
datacube_no2 = p$resample_spatial(
data = datacube_no2, resolution = 10/111
)
# mask for cloud cover
threshold_ <- function(data, context) {
threshold <- p$gte(data[1], value)
return(threshold)
}
# apply the threshold to the cube
cloud_threshold <- p$apply(data = datacube, process = threshold_)
# mask the cloud cover with the calculated mask
datacube <- p$mask(datacube_no2, cloud_threshold)
# interpolate where nodata
interpolate = function(data,context) {
return(p$array_interpolate_linear(data = data))
}
datacube = p$apply_dimension(process = interpolate,
data = datacube, dimension = "t"
)
# apply dimension to time
datacube = p$apply_dimension(data = datacube, dimension = "t")
# compress spatial dimension
lon = c(w, e)
lat = c(s, n)
bbox_df = data.frame(lon, lat)
pol = st_polygon(
list(
cbind(
bbox_df$lon[c(1,2,2,1,1)],
bbox_df$lat[c(1,1,2,2,1)])
)
)
polygons = st_sfc(pol, crs=4326)
polygons = st_sf(polygons)
# add any attribute as a workaround
polygons$anAttribute <- 4
# aggregate spatially
datacube_mean <- p$aggregate_spatial(data = datacube, reducer = function(data, context) { p$mean(data) }, geometries = polygons)
datacube_max <- p$aggregate_spatial(data = datacube, reducer = function(data, context) { p$max(data) }, geometries = polygons)
graph = as(datacube_mean,"Graph")
compute_result(graph = graph, output_file = "time-series-mean.json")
graph = as(datacube_max,"Graph")
compute_result(graph = graph, output_file = "time-series-max.json")
print("max time series stored \n")
# read json - add save option
ts_mean = fromJSON(file = "time-series-mean.json")
ls()
graph
compute_result(graph = graph, output_file = "time-series-mean.json")
ls()
ls(graph)
compute_result(graph = graph, output_file = "time-series-mean.json")
graph = as(datacube_mean,"Graph")
compute_result(graph = graph, output_file = "time-series-mean.json")
# User defined process
p = processes()
w = 6.09
s = 46.15
e = 6.99
n = 46.57
# Talinn
w = 24.52
s = 59.35
e = 24.96
n = 59.50
# Sarajevo
w = 18.23
s = 43.81
e = 18.54
n = 43.98
## time extent
date1 = "2018-07-01"
date2 = "2018-07-31"
## cloud cover value (>=)
value = 0.5
# acquire data for the extent
datacube_no2 = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=c(date1, date2),
bands=c("NO2")
)
datacube = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=c(date1, date2),
bands=c("CLOUD_FRACTION")
)
# 10km x 10km grid : may be optional
datacube = p$resample_spatial(
data = datacube, resolution = 10/111
)
datacube_no2 = p$resample_spatial(
data = datacube_no2, resolution = 10/111
)
# mask for cloud cover
threshold_ <- function(data, context) {
threshold <- p$gte(data[1], value)
return(threshold)
}
# apply the threshold to the cube
cloud_threshold <- p$apply(data = datacube, process = threshold_)
# mask the cloud cover with the calculated mask
datacube <- p$mask(datacube_no2, cloud_threshold)
# interpolate where nodata
interpolate = function(data,context) {
return(p$array_interpolate_linear(data = data))
}
datacube = p$apply_dimension(process = interpolate,
data = datacube, dimension = "t"
)
###
# moving average
moving_average <- function(data, context) {
return(p$filter_temporal(data = data, dimension = "t"))
}
datacube = p$apply_dimension(process = moving_average,
data = datacube, dimension = "t"
)
###
# apply dimension to time
datacube = p$apply_dimension(data = datacube, dimension = "t")
# compress spatial dimension
lon = c(w, e)
lat = c(s, n)
bbox_df = data.frame(lon, lat)
pol = st_polygon(
list(
cbind(
bbox_df$lon[c(1,2,2,1,1)],
bbox_df$lat[c(1,1,2,2,1)])
)
)
polygons = st_sfc(pol, crs=4326)
polygons = st_sf(polygons)
# add any attribute as a workaround
polygons$anAttribute <- 4
# aggregate spatially
datacube_mean <- p$aggregate_spatial(data = datacube, reducer = function(data, context) { p$mean(data) }, geometries = polygons)
datacube_max <- p$aggregate_spatial(data = datacube, reducer = function(data, context) { p$max(data) }, geometries = polygons)
w = 6.09
s = 46.15
e = 6.99
n = 46.57
## time extent
date1 = "2018-07-01"
date2 = "2018-07-31"
## cloud cover value (>=)
value = 0.5
# acquire data for the extent
datacube_no2 = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=c(date1, date2),
bands=c("NO2")
)
datacube = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=c(date1, date2),
bands=c("CLOUD_FRACTION")
)
# 10km x 10km grid : may be optional
datacube = p$resample_spatial(
data = datacube, resolution = 10/111
)
datacube_no2 = p$resample_spatial(
data = datacube_no2, resolution = 10/111
)
# mask for cloud cover
threshold_ <- function(data, context) {
threshold <- p$gte(data[1], value)
return(threshold)
}
# apply the threshold to the cube
cloud_threshold <- p$apply(data = datacube, process = threshold_)
# mask the cloud cover with the calculated mask
datacube <- p$mask(datacube_no2, cloud_threshold)
# interpolate where nodata
interpolate = function(data,context) {
return(p$array_interpolate_linear(data = data))
}
datacube = p$apply_dimension(process = interpolate,
data = datacube, dimension = "t"
)
# apply dimension to time
datacube = p$apply_dimension(data = datacube, dimension = "t")
# compress spatial dimension
lon = c(w, e)
lat = c(s, n)
bbox_df = data.frame(lon, lat)
pol = st_polygon(
list(
cbind(
bbox_df$lon[c(1,2,2,1,1)],
bbox_df$lat[c(1,1,2,2,1)])
)
)
polygons = st_sfc(pol, crs=4326)
polygons = st_sf(polygons)
# add any attribute as a workaround
polygons$anAttribute <- 4
# aggregate spatially
datacube_mean <- p$aggregate_spatial(data = datacube, reducer = function(data, context) { p$mean(data) }, geometries = polygons)
datacube_max <- p$aggregate_spatial(data = datacube, reducer = function(data, context) { p$max(data) }, geometries = polygons)
graph = as(datacube_mean,"Graph")
compute_result(graph = graph, output_file = "time-series-mean.json")
?compute_result
?graph
graph
compute_result(graph = graph, output_file = "time-series-mean.json")
con = connect(host = "https://openeo.cloud")
# Collections
collections = list_collections()
# Processes
processes = list_processes()
# login
login()
